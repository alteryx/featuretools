name: Unit Tests - Latest Dependencies
on:
  pull_request:
    types: [opened, synchronize]
  push:
    branches:
      - main
  workflow_dispatch:
jobs:
  unit_tests:
    name: ${{ matrix.python_version }} unit tests ${{ matrix.libraries }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python_version: ["3.8"]
        libraries: ["core", "spark - misc", "spark - computational", "spark - entityset_1", "spark - entityset_2", "spark - primitives"]

    steps:
      - name: Set up python ${{ matrix.python_version }}
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.python_version }}
      - name: Checkout repository
        uses: actions/checkout@v2
        with:
          ref: ${{ github.event.pull_request.head.ref }}
          repository: ${{ github.event.pull_request.head.repo.full_name }}
      - name: Build featuretools package
        run: make package_featuretools
      - name: Set up pip and graphviz
        run: |
          pip config --site set global.progress_bar off
          python -m pip install --upgrade pip
          sudo apt update && sudo apt install -y graphviz
      - if: ${{ !startsWith(matrix.libraries, 'spark') }}
        name: Install featuretools with test requirements
        run: |
          python -m pip install unpacked_sdist/
          python -m pip install unpacked_sdist/[test]
      - if: ${{ startsWith(matrix.libraries, 'spark') }}
        name: Install spark pkg, featuretools with test requirements and spark requirements
        run: |
          sudo apt install -y openjdk-11-jre-headless
          JAVA_HOME="/usr/lib/jvm/java-11-openjdk-amd64"
          python -m pip install unpacked_sdist/[spark]
          python -m pip install unpacked_sdist/[test]
      - if: ${{ matrix.python_version == 3.8 && startsWith(matrix.libraries, 'spark') }}
        name: Generate coverage args
        run: echo "coverage_args=--cov=featuretools --cov-config=../pyproject.toml --cov-report=xml:../coverage.xml" >> $GITHUB_ENV
      - if: ${{ env.coverage_args }}
        name: Erase coverage files
        run: |
          cd unpacked_dist
          coverage erase
      - if: ${{ !startsWith(matrix.libraries, 'spark') }}
        name: Run unit tests (no code coverage)
        run: |
          cd unpacked_dist
          pytest featuretools/ -n=2
      - if: ${{ matrix.libraries == 'spark - misc' }}
        name: Run unit tests (misc)
        run: |
          cd unpacked_dist
          pytest featuretools/ -n=2 --ignore=featuretools/tests/computational_backend --ignore=featuretools/tests/entityset_tests --ignore=featuretools/tests/primitive_tests ${{ env.coverage_args }}
      - if: ${{ matrix.libraries == 'spark - computational' }}
        name: Run unit tests (computational backend)
        run: |
          cd unpacked_dist
          pytest featuretools/tests/computational_backend/ -n=2 ${{ env.coverage_args }}
      - if: ${{ matrix.libraries == 'spark - entityset_1' }}
        name: Run unit tests (entityset batch 1)
        run: |
          cd unpacked_dist
          pytest featuretools/tests/entityset_tests -n=2 --ignore=featuretools/tests/entityset_tests/test_es.py --ignore=featuretools/tests/entityset_tests/test_ww_es.py ${{ env.coverage_args }}
      - if: ${{ matrix.libraries == 'spark - entityset_2' }}
        name: Run unit tests (entityset batch 2)
        run: |
          cd unpacked_dist
          pytest featuretools/tests/entityset_tests/test_es.py featuretools/tests/entityset_tests/test_ww_es.py -n=2 ${{ env.coverage_args }}
      - if: ${{ matrix.libraries == 'spark - primitives' }}
        name: Run unit tests (primitives)
        run: |
          cd unpacked_dist
          pytest featuretools/tests/primitive_tests -n=2 ${{ env.coverage_args }}
      - if: ${{ env.coverage_args }}
        name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          fail_ci_if_error: true
          files: ${{ github.workspace }}/coverage.xml
          verbose: true
