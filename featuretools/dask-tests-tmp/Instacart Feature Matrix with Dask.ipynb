{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Featuretools with Dask to generate a feature matrix\n",
    "\n",
    "<p style=\"margin:30px\">\n",
    "    <img width=50% src=\"https://www.featuretools.com/wp-content/uploads/2017/12/FeatureLabs-Logo-Tangerine-800.png\" alt=\"Featuretools\" />\n",
    "</p>\n",
    "\n",
    "In this tutorial, we show how [Featuretools](http://www.featuretools.com) can be used to perform feature engineering on a multi-table dataset of 3 million online grocery orders provided by Instacart. We will generate a feature matrix that can be used to train a machine learning model to predict what product a customer buys next.\n",
    "\n",
    "*Note: This notebook requires a dataset from Instacart. You can download the dataset [here](https://www.instacart.com/datasets/grocery-shopping-2017). Once you have downloaded the data, be sure to place the CSV files contained in the archive in a directory called `data/instacart`. If you use a different directory name, you will need to update the code below to point to the proper location.*\n",
    "\n",
    "## Highlights\n",
    "\n",
    "* We demonstrate how to generate features in a scalable manner using [Dask](http://dask.pydata.org/en/latest/)\n",
    "* We automatically generate label times using [Compose](https://github.com/FeatureLabs/compose) which can be reused for numerous prediction problems\n",
    "* We save the resulting feature matrix to disk so it can be used in downstream machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import composeml as cp\n",
    "import featuretools as ft\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import Client\n",
    "ft.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load and preprocess data\n",
    "\n",
    "First, we will create a Dask distributed client so we can track the progress of our computation on the Dask dashboard that is created when the client is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will specify our input and output directories and set the blocksize we will be using to read the raw CSV files into Dask dataframes. When running on a 4 processor machine with 16GB of memory available for the Dask workers, a `40MB` blocksize has worked well. This number may need to be adjusted based on your specific environment. Refer to the [Dask documentation](https://docs.dask.org/en/latest/best-practices.html) for additional info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"data\", \"instacart\")\n",
    "output_dir = os.path.join(\"data\", \"instacart\", \"dask_data\")\n",
    "blocksize = \"40MB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will read our data into Dask dataframes. This operation will complete quite fast as we are not actually bringing the data into memory at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products = dd.concat([dd.read_csv(os.path.join(data_dir, \"order_products__prior.csv\"), blocksize=blocksize),\n",
    "                            dd.read_csv(os.path.join(data_dir, \"order_products__train.csv\"), blocksize=blocksize)])\n",
    "orders = dd.read_csv(os.path.join(data_dir, \"orders.csv\"), blocksize=blocksize)\n",
    "departments = dd.read_csv(os.path.join(data_dir, \"departments.csv\"), blocksize=blocksize)\n",
    "products = dd.read_csv(os.path.join(data_dir, \"products.csv\"), blocksize=blocksize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few cells, we will perform some required preprocessing to clean up our data. We will merge together some of the raw dataframes and add absolute order time information from the relative times used in the raw data. This will allow us to use cutoff times as part of the Deep Feature Synthesis process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products = order_products.merge(products).merge(departments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time(df):\n",
    "    df.reset_index(drop=True)\n",
    "    df[\"order_time\"] = np.nan\n",
    "    days_since = df.columns.tolist().index(\"days_since_prior_order\")\n",
    "    hour_of_day = df.columns.tolist().index(\"order_hour_of_day\")\n",
    "    order_time = df.columns.tolist().index(\"order_time\")\n",
    "\n",
    "    df.iloc[0, order_time] = pd.Timestamp('Jan 1, 2015') +  pd.Timedelta(df.iloc[0, hour_of_day], \"h\")\n",
    "    for i in range(1, df.shape[0]):\n",
    "        df.iloc[i, order_time] = df.iloc[i - 1, order_time] \\\n",
    "            + pd.Timedelta(df.iloc[i, days_since], \"d\") \\\n",
    "                                    + pd.Timedelta(df.iloc[i, hour_of_day], \"h\")\n",
    "\n",
    "    to_drop = [\"order_number\", \"order_dow\", \"order_hour_of_day\", \"days_since_prior_order\", \"eval_set\"]\n",
    "    df.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = orders.groupby(\"user_id\").apply(add_time)\n",
    "order_products = order_products.merge(orders[[\"order_id\", \"order_time\"]])\n",
    "order_products[\"order_product_id\"] = order_products[\"order_id\"] * 1000 + order_products[\"add_to_cart_order\"]\n",
    "order_products = order_products.drop([\"product_id\", \"department_id\", \"add_to_cart_order\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the preprocessing work is complete, we will save the results to disk. This will allow us to start from this point in the process in the future, without having to repeat all of the preprocessing steps. If you have already saved the results to disk previously, you can skip the cell below.\n",
    "\n",
    "#### Note: The process of saving to CSV is computationally intensive and may take 45 minutes or more, depending on the system you are using. You can use the Dask dashboard to monitor the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.to_csv(os.path.join(output_dir, \"orders-*.csv\"), index=False)\n",
    "order_products.to_csv(os.path.join(output_dir, \"order_products-*.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already performed the preprocessing steps and saved the processed files to disk, you can read them in with the commands in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = dd.read_csv(os.path.join(output_dir, \"orders-*.csv\"), blocksize=blocksize)\n",
    "order_products = dd.read_csv(os.path.join(output_dir, \"order_products-*.csv\"), blocksize=blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a Featuretools entityset\n",
    "\n",
    "When using Dask dataframes to create an entityset, variable type inference is not performed as it is with entitysets created from pandas dataframes. As a result, users must specify the Featuretools variable types for all of the columns in the dataframes that make up the entityset when using Dask. In the following cell we define the data types for the `order_products` and `orders` entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products_vtypes = {\n",
    "    \"order_id\": ft.variable_types.Id,\n",
    "    \"reordered\": ft.variable_types.Boolean,\n",
    "    \"product_name\": ft.variable_types.Categorical,\n",
    "    \"aisle_id\": ft.variable_types.Categorical,\n",
    "    \"department\": ft.variable_types.Categorical,\n",
    "    \"order_time\": ft.variable_types.Datetime,\n",
    "    \"order_product_id\": ft.variable_types.Index,\n",
    "}\n",
    "\n",
    "order_vtypes = {\n",
    "    \"order_id\": ft.variable_types.Index,\n",
    "    \"user_id\": ft.variable_types.Id,\n",
    "    \"order_time\": ft.variable_types.DatetimeTimeIndex,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the data types, we can create the entityset and establish the relationship between the two entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = ft.EntitySet(\"instacart\")\n",
    "es.entity_from_dataframe(entity_id=\"order_products\",\n",
    "                         dataframe=order_products,\n",
    "                         index=\"order_product_id\",\n",
    "                         variable_types=order_products_vtypes,\n",
    "                         time_index=\"order_time\")\n",
    "\n",
    "es.entity_from_dataframe(entity_id=\"orders\",\n",
    "                         dataframe=orders,\n",
    "                         index=\"order_id\",\n",
    "                         variable_types=order_vtypes,\n",
    "                         time_index=\"order_time\")\n",
    "\n",
    "es.add_relationship(ft.Relationship(es[\"orders\"][\"order_id\"], es[\"order_products\"][\"order_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will normalize the `orders` entity to create a new `users` entity that we will later use as the target entity during the deep feature synthesis process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.normalize_entity(base_entity_id=\"orders\", new_entity_id=\"users\", index=\"user_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish up creation of the entity set we will add last time indexes and set some interesting values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.add_last_time_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es[\"order_products\"][\"department\"].interesting_values = ['produce', 'dairy eggs', 'snacks', 'beverages', 'frozen', 'pantry', 'bakery', 'canned goods', 'deli', 'dry goods pasta']\n",
    "es[\"order_products\"][\"product_name\"].interesting_values = ['Banana', 'Bag of Organic Bananas', 'Organic Baby Spinach', 'Organic Strawberries', 'Organic Hass Avocado', 'Organic Avocado', 'Large Lemon', 'Limes', 'Strawberries', 'Organic Whole Milk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Use Compose to generate our cutoff times dataframe\n",
    "\n",
    "In the cells that follow we will demonstrate how [Compose](https://github.com/FeatureLabs/compose) can be used to generate the label times dataframe that will be used as cutoff times for deep feature synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bought_product(df, product_name):\n",
    "    purchased = df.product_name.str.contains(product_name).any()\n",
    "    return purchased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = cp.LabelMaker(\n",
    "    target_entity='user_id',\n",
    "    time_index='order_time',\n",
    "    labeling_function=bought_product,\n",
    "    window_size='4w',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(es):\n",
    "    df = es['order_products'].df.merge(es['orders'].df).merge(es['users'].df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compose does not currently work on Dask dataframes, so we must first run `.compute()` on the denormalized entityset to switch to pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = denormalize(es).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_times = lm.search(\n",
    "    df.sort_values('order_time'),\n",
    "    minimum_data='2015-03-15',\n",
    "    num_examples_per_instance=2,\n",
    "    product_name='Banana',\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Run Deep Feature Synthesis\n",
    "\n",
    "At this point we are ready to run deep feature synthesis to generate our feature matrix. This will execute quickly and the resulting feature matrix will be returned as a Dask dataframe. This process does not cause the feature matrix to be computed or brought into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix, features = ft.dfs(target_entity=\"users\",\n",
    "                                  cutoff_time=label_times,\n",
    "                                  entityset=es,\n",
    "                                  trans_primitives=[\"day\", \"year\", \"month\", \"weekday\", \"num_words\", \"num_characters\"],\n",
    "                                  agg_primitives=[\"sum\", \"std\", \"max\", \"min\", \"mean\", \"count\", \"percent_true\"],\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a Dask feature matrix, we can save it to disk for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.to_csv(os.path.join(output_dir, \"feature_matrix-*.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's read back in the feature matrix we just saved, compute it and take a look at what we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = dd.read_csv(os.path.join(output_dir, \"feature_matrix-*.csv\")).compute()\n",
    "fm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature matrix memory usage: {fm.memory_usage().sum() / 1000000} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finished, we can close our Dask client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('env': venv)",
   "language": "python",
   "name": "python37464bitenvvenvb0576f6089f14bba82265783ef39e793"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
